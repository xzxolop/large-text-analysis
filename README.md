# large-text-analysis

## Идея приложения

Необходимо провести что-то на подобии кластерного анализа текстов. На вход программе передается слово на английском языке. Например слово data. Далее я получаю слова в контексте с которыми это слово встречается чаще всего:

data
> big 1000 совпадений
> python 200 совпадений

далее я могу раскрыть один из пунктов и получить следующие слова с которыми чаще всего идут совпадения. Например нажав на big: 

data
> big 1000
    > job 500
    > ML 200
    > ...
> python 200

Мой текст разбивается на sentance (предложения) используя sentance tokenization, world tokenization. Каждое предложение будем независимо обрабатывать с помощью TF-IDF индекса, а также используем ElasticSearch. 

Демонстрацию планируется написать с использованием streamlit.

## Запуск проекта

### С использованием uv

uv sync

uv run ./run.py

### С использованием pip
Backend:

bash
```
pip install -r requirements.txt
python run.py

```

### Запуск демонстрации в консоли

```
python .\console-demo\main.py
```